[[scaling-on-kubernetes]]
= Scaling {agent} on kubernetes

For more information on how to deploy Elastic Agent on Kubernetes please advise the pages below:

- How to run the {agent} on <<running-on-kubernetes-managed-by-fleet>>.
- How to run the {agent} on <<running-on-kubernetes-standalone>>.

[discrete]
== Observability at scale

This document will try to summarize the key factors and best practices for using https://www.elastic.co/guide/en/welcome-to-elastic/current/getting-started-kubernetes.html[Elastic Observability] to monitor Kubernetes infrastructure at scale. Users need to consider different parameters and adjust Elastic Stack accordingly. The following elements are being affected as the size of Kubernetes cluster increases:

- The amount of metrics being collected from several kubernetes endpoints
- The Agent's resources to cope with the high CPU and Memory needs for the internal processing
- The ElasticSearch resources needed due to the higher rate of metric ingestion
- The Dashboard's Visualisations response times as more data are requested on a given time window 

The document is divided in two main sections:

- <<configuration-practices,Configuration practices>>
- <<validation-and-troubleshooting-practices,Validation and Troubleshooting methods>>

[discrete]
[[configuration-practices]]
== Configuration Practices

[discrete]
=== Configure Agent Resources

The kubernetes observability is based on https://docs.elastic.co/en/integrations/kubernetes[kubernetes integration], which is fetching metrics from several components:

- **Per node:**
  - kubelet
  - controller-manager
  - scheduler
  - proxy
- **Cluster Wide (i.e. unique metrics for the whole cluster):**
  - kube-state-metrics
  - apiserver

The default manifest provided deploys elastic agent as DaemonSet which results in an elastic agent being deployed on every node of the Kubernetes cluster. 

Additionally, by default one agent is elected as **leader**(for more information visit <<kubernetes_leaderelection-provider>>). This is configured in our https://github.com/elastic/elastic-agent/blob/main/deploy/kubernetes/elastic-agent-standalone-kubernetes.yaml#L32[manifests]. The Agent Pod which holds the leadership lock will be responsible for also collecting the cluster wide metrics in addition to its node's metrics.

--
[role="screenshot"]
image::../images/k8sscaling.png[Agent as daemonset]
--

The above schema explains how {agent} collects and sends metrics to Elasticsearch. Because of Leader Agent being responsible to also collecting cluster-lever metrics, this means that it requires additional resources.

The DaemonSet deployment approach with leader election simplifies the installation of the {agent} because we define less Kubernetes Resources in our manifest and we only need one single Agent policy for our Agents. Hence it is the default supported method for <<running-on-kubernetes-managed-by-fleet, Managed Elastic Agent installation>>


[discrete]
=== Specifying resources and limits in Agent manifests

Based on our tests we advise to configure only the `limit` section of the `resources` section in the manifest. In this way the `request`'s settings of the `resources` will fall back to the `limits` specified. The `limits` is the upper bound limit of your microservice process, meaning that can operate in less resources and protect Kubernetes to assign bigger usage and protect from possible resource exhaustion.

[source,yaml]
------------------------------------------------
resources:
    limits:
      cpu: "1000m"
      memory: "200Mi"
------------------------------------------------


Based on our https://github.com/elastic/elastic-agent/blob/7467fd72bccab9a0e1b0adc1761cde8520390943/docs/elastic-agent-scaling-tests.md[tests], the following table provides guidelines to adjust {agent} limits on different kubernetes sizes:

Sample Elastic Agent Configurations:
|===
| No of Pods in K8s Cluster | Leader Agent Resources | Rest of Agents 
| 1000   | cpu: "1500m",  memory: "800Mi" | cpu: "300m",  memory: "600Mi" 
| 3000   | cpu: "2000m",  memory: "1500Mi" | cpu: "400m",  memory: "800Mi" 
| 5000   | cpu: "3000m",  memory: "2500Mi" | cpu: "500m",  memory: "900Mi" 
| 10000  | cpu: "3000m",  memory: "3600Mi" | cpu: "700m",  memory: "1000Mi" 
|===

> The above tests were performed with {agent} version 8.7 and scraping period of `10sec` (period setting for the Kubernetes integration). Those numbers are just indicators and should be validated for each different kubernetes environment and amount of workloads.

[discrete]
=== Proposed Agent Installations for large scale

Although daemonset installation is simple, it can not accomodate the varying agent resource requirements depending on the collected metrics. The need for appropriate resource assignment at large scale requires more granular installation methods.

Elastic Agent deployment is broken in groups as follows:
- a dedicated elastic agent deployment of a single agent for collecting cluster wide metrics from the apiserver
- Node level elastic agents(no leader agent) in a Daemonset 
- kube-state-metrics shards and elastic agents in the StatefulSet defined in the kube-state-metrics autosharding manifest
 
Each of these groups of elastic agents will have its own policy specific to its function and can be resourced independently in the appropriate manifest to accomodate its specific resource requirements.

resource assignment led us to alternatives installation methods. 

IMPORTANT: The main suggestion for big scale clusters *is to install Elastic Agent as side container with Kube State Metrics Shard*. The installation is explained in details https://github.com/elastic/elastic-agent/tree/909b09def863205ae928f440d04851494c8e9933/docs/manifests/kustomize-autosharding[here]

The following **3 alternative configuration methods** have been verified:

1. With `hostNetwork:false` 
  - Elastic Agent as Side Container within KSM Shard pod
  - For non-leader Elastic Agent deployments that collect per KSM shards
2. With `podAntiAffinity` to isolate the Elastic Agent daemonset pods from rest of deployments
3. With `taint/tolerations` to isolate the Elastic Agent daemonset pods from rest of deployments

For more information, please advise https://github.com/elastic/elastic-agent/blob/ksmsharding/docs/elastic-agent-ksm-sharding.md[following guide]

Based on our https://github.com/elastic/elastic-agent/blob/7467fd72bccab9a0e1b0adc1761cde8520390943/docs/elastic-agent-scaling-tests.md[tests], the following table provides number of KSM sharding configuration per different kubernetes sizes:

The following table aims to assist users on how to configure their KSM Sharding as Kubernetes cluster scales:
|===
| No of Pods in K8s Cluster | No of KSM Shards | Agent Resources
| 1000   | No Sharding can be handled with default KSM config | limits: memory: 700Mi , cpu:500m 
| 3000   | 4 Shards | limits: memory: 1400Mi , cpu:1500m 
| 5000   | 6 Shards | limits: memory: 1400Mi , cpu:1500m 
| 10000  | 8 Shards | limits: memory: 1400Mi , cpu:1500m 
|===

> The tests above were performed with {agent} version 8.8 + TSDB Enabled and scraping period of `10sec` (for the Kubernetes integration). Those numbers are just indicators and should be validated per different kubernetes policy configuration, along with applications that the kubernetes cluster might include

NOTE: Tests have run until 10K pods per cluster. Scaling to bigger number of pods might require additional confguration from Kubernetes Side and Cloud Providers but the basic idea of installing Elastic Agent while horizontally scaling KSM remains the same

[discrete]
=== Agent Scheduling

Parallel to {agent} resource specification problem, the scheduling of agents is an other common problem that users phase as kubernetes cluster is growing. Trying to prioritise the agent installation before rest of application microservices, https://github.com/elastic/elastic-agent/blob/main/docs/manifests/elastic-agent-managed-gke-autopilot.yaml#L8-L16[PriorityClasses suggested]

[discrete]
=== Kubernetes Policy Configuration

Policy configuration of kubernetes package can heavily affect the amount of metrics collected and finally ingested. Factors that should be considered in order to make your collection and ingestin lighter:

- Scraping period of Kubernetes endpoints
- Disabling log collection
  - Keep audit logs disabled
- Disable events dataset
- Disable Kubernetes control plane datasets in Cloud managed kubernetes instances (see more info ** <<running-on-gke-managed-by-fleet>>, <<running-on-eks-managed-by-fleet>>, <<running-on-aks-managed-by-fleet>> pages)

User experience regarding Dashboard responses is also affected from the size of data being requested. As dashbords can contain multiple visualisations, the general conisderation is to split visualisasations and group them according to the frequency of access. The less number of visualisations tends to improve user experience.

Additionally, https://github.com/elastic/integrations/blob/main/docs/dashboard_guidelines.md[Dashboard Guidelines] is constantly updated also to track needs of observability at scale.

[discrete]
=== Elastic Stack Configuration

The configuration of Elastic Stack needs to be taken under consideration in large scale deployments. In case of Elastic Cloud deployments the choice of the deployment https://www.elastic.co/guide/en/cloud/current/ec-getting-started-profiles.html[hardware profile] is important. 

For heavy processing and big ingestion rate needs, the `CPU-optimised` profile is proposed.

[discrete]
[[validation-and-troubleshooting-practices]]
== Validation and Troubleshooting practices

[discrete]
=== Define if Agents are collecting as expected 

After {agent} deployment, we need to verify that agent services are healthy, not restarting (stability) and that collection of metrics continues with expected rate (latency).

**For stability:**

If {agent} is configured as managed, in {kib} you can observe under **Fleet>Agents**

--
[role="screenshot"]
image::../images/agent-status.png[Agent Status]
--

Additionally you can verify the process status with following commands:

[source,bash]
------------------------------------------------
kubectl get pods -A | grep elastic
kube-system   elastic-agent-ltzkf                        1/1     Running   0          25h
kube-system   elastic-agent-qw6f4                        1/1     Running   0          25h
kube-system   elastic-agent-wvmpj                        1/1     Running   0          25h
------------------------------------------------

Find leader agent:


[source,bash]
------------------------------------------------
❯ k get leases -n kube-system | grep elastic
NAME                                      HOLDER                                                                       AGE
elastic-agent-cluster-leader   elastic-agent-leader-elastic-agent-qw6f4                                     25h
------------------------------------------------

Exec into Leader agent and verify the process status:

[source,bash]
------------------------------------------------
❯ kubectl exec -ti -n kube-system elastic-agent-qw6f4 -- bash
root@gke-gke-scaling-gizas-te-default-pool-6689889a-sz02:/usr/share/elastic-agent# ./elastic-agent status
State: HEALTHY
Message: Running
Fleet State: HEALTHY
Fleet Message: (no message)
Components:
  * kubernetes/metrics  (HEALTHY)
                        Healthy: communicating with pid '42423'
  * filestream          (HEALTHY)
                        Healthy: communicating with pid '42431'
  * filestream          (HEALTHY)
                        Healthy: communicating with pid '42443'
  * beat/metrics        (HEALTHY)
                        Healthy: communicating with pid '42453'
  * http/metrics        (HEALTHY)
                        Healthy: communicating with pid '42462'
------------------------------------------------

It is a common problem of lack of CPU/memory resources that agent process restart as kubernetes size grows. In the logs of agent you 

[source,json]
------------------------------------------------
kubectl logs -n kube-system elastic-agent-qw6f4 | grep "kubernetes/metrics"
[ouptut truncated ...]

(HEALTHY->STOPPED): Suppressing FAILED state due to restart for '46554' exited with code '-1'","log":{"source":"elastic-agent"},"component":{"id":"kubernetes/metrics-default","state":"STOPPED"},"unit":{"id":"kubernetes/metrics-default-kubernetes/metrics-kube-state-metrics-c6180794-70ce-4c0d-b775-b251571b6d78","type":"input","state":"STOPPED","old_state":"HEALTHY"},"ecs.version":"1.6.0"}
{"log.level":"info","@timestamp":"2023-04-03T09:33:38.919Z","log.origin":{"file.name":"coordinator/coordinator.go","file.line":861},"message":"Unit state changed kubernetes/metrics-default-kubernetes/metrics-kube-apiserver-c6180794-70ce-4c0d-b775-b251571b6d78 (HEALTHY->STOPPED): Suppressing FAILED state due to restart for '46554' exited with code '-1'","log":{"source":"elastic-agent"}

------------------------------------------------

You can verify the instant resource consumption by running `top pod` command and indentify if agents are close to the limits you have specified in your manifest. 

[source,bash]
------------------------------------------------
kubectl top pod  -n kube-system | grep elastic
NAME                                                             CPU(cores)   MEMORY(bytes)
elastic-agent-ltzkf                                              30m          354Mi
elastic-agent-qw6f4                                              67m          467Mi
elastic-agent-wvmpj                                              27m          357Mi
------------------------------------------------

[discrete]
=== Verify Ingestion Latency

Kibana Discovery can be used to identify frequency of your metrics being ingested.

Filter for Pod dataset:
--
[role="screenshot"]
image::../images/pod-latency.png[Pod Metricser]
--

Filter for State_Pod dataset
--
[role="screenshot"]
image::../images/state-pod.png[Kubernetes State Pod Metricser]
--

Identify how many events have been sent to Elasticsearch:

[source,bash]
------------------------------------------------
kubectl logs -n kube-system elastic-agent-h24hh -f | grep -i state_pod 
[ouptut truncated ...]

"state_pod":{"events":2936,"success":2936}
------------------------------------------------

The number of events denotes the number of documents that should be depicted inside Kibana Discovery page.

> For eg, in a cluster with 798 pods, then 798 docs should be depicted in block of ingestion inside {kib}


[discrete]
=== Define if Elasticsearch is the bottleneck of ingestion

In some cases maybe the Elasticsearch can not cope with the rate of data that are trying to be ingested. In order to verify the resource utilisation the installation of [Monitoring Cluster](https://www.elastic.co/guide/en/elasticsearch/reference/current/monitoring-overview.html) is advised

Additionally, in Elastic Cloud deployments you can navigate to *Manage Deployment > Deployments > Monitoring > Performance*.
Corresponding dashboards for `CPU Usage`, `Index Response Times` and `Memory Pressure` can reveal possible problems and suggest vertical scaling of Elastic Stack resources.



== Relevant links

- https://www.elastic.co/guide/en/welcome-to-elastic/current/getting-started-kubernetes.html[Monitor Kubernetes Infrastructure]
- https://www.elastic.co/blog/kubernetes-cluster-metrics-logs-monitoring[Blog: Managing your Kubernetes cluster with Elastic Observability]
